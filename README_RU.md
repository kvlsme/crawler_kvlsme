# crawler_kvlsme
Шаблон для краулера на основе scrapy-crawler

## Установка

1. Установите Scrapy:
    ```bash
    pip3 install scrapy
    ```

2. Создайте новый проект Scrapy:
    ```bash
    scrapy startproject name_of_your_project
    ```

3. Запустите краулер:
    ```bash
    scrapy crawl template
    ```

## Использование

1. Измените `start_urls` в `template.py`, чтобы включить URL категорий, которые вы хотите сканировать.
2. Настройте выражения XPath в методах `parse` и `parse_product`, чтобы они соответствовали структуре целевого сайта.
3. Запустите краулер с помощью команды:
    ```bash
    scrapy crawl template
    ```

## Зависимости

- Python 3.x
- Scrapy

## Особые примечания

- Убедитесь, что выражения XPath в файле `template.py` обновлены в соответствии со структурой сайта, который вы сканируете.
- Вывод сохраняется в CSV файл под названием `template.csv`. Вы можете изменить это имя в методе `__init__` класса `Template`.

## Структура кода

- `template.py`: Содержит код паука Scrapy. Определяет имя краулера, начальные URL и логику парсинга.
    - `start_urls`: Список URL для начала сканирования.
    - `parse`: Метод для парсинга страниц категорий и извлечения ссылок на продукты.
    - `parse_product`: Метод для парсинга отдельных страниц продуктов и извлечения необходимых полей.
    - `get_next_page_url`: Метод для генерации URL для следующей страницы.

## Контрибьюторы

- Заполнитель для контрибьюторов

## Лицензия

Этот проект лицензирован на условиях Mozilla Public License Version 2.0. См. файл [LICENSE](LICENSE) для получения деталей.
